{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.cluster import homogeneity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = '/home/abhishek/dev/Semester_2/SMAI/Assignments/Assignment_2/Datasets/Question-6/dataset/'\n",
    "test_url = '/home/abhishek/dev/Semester_2/SMAI/Assignments/Assignment_2/Datasets/Question-6/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(url):\n",
    "    files = os.listdir(url)\n",
    "    # print(len(files))\n",
    "    doc = []\n",
    "    label = []\n",
    "    for fname in files:\n",
    "        x = re.split(\"_\", fname)\n",
    "        y = re.split(\"\\.\",x[1])\n",
    "        label.append(int(y[0]))\n",
    "        with open(url+fname,\"r\") as f:\n",
    "            data = f.read().replace('\\n','')\n",
    "            data = data.translate(str.maketrans('', '', string.punctuation+string.digits))\n",
    "            stemmer = PorterStemmer()\n",
    "            input_str = word_tokenize(data)\n",
    "            result = \"\"\n",
    "            for word in input_str:\n",
    "                result = result+\" \"+stemmer.stem(word)\n",
    "            doc.append(result)\n",
    "    return label, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_files(url):\n",
    "    files = os.listdir(url)\n",
    "    doc = []\n",
    "    for fname in files:\n",
    "        with open(url+fname,\"r\") as f:\n",
    "            data = f.read().replace('\\n','')\n",
    "            data = data.translate(str.maketrans('', '', string.punctuation+string.digits))\n",
    "            stemmer = PorterStemmer()\n",
    "            input_str = word_tokenize(data)\n",
    "            result = \"\"\n",
    "            for word in input_str:\n",
    "                result = result+\" \"+stemmer.stem(word)\n",
    "            doc.append(result)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cluster_median(cluster):\n",
    "    print(\"calculating cluster centroid........\")\n",
    "    X_train_raw = pd.DataFrame(X_train.todense())\n",
    "    new_centroid = [[],[],[],[],[]]\n",
    "    for i in range(0,len(cluster)):\n",
    "        new_centroid[i].append(X_train_raw.iloc[cluster[i]].mean(axis=0))\n",
    "    return new_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intial_cluster_centroid():\n",
    "    cluster_centroid = [[],[],[],[],[]]\n",
    "    return cluster_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_cluster_centroid_random():\n",
    "    X_train_raw = X_train.todense()\n",
    "    total_points = X_train.shape[0]\n",
    "    val_range = total_points/5\n",
    "    fi = random.randint(0,val_range)\n",
    "    si = random.randint(val_range,val_range*2)\n",
    "    ti = random.randint(val_range*2,val_range*3)\n",
    "    fo = random.randint(val_range*3,val_range*4)\n",
    "    ft = random.randint(val_range*4,val_range*5)        \n",
    "    cluster_centroid = [X_train_raw[fi],X_train_raw[si],X_train_raw[ti],X_train_raw[fo],X_train_raw[ft]]\n",
    "    return cluster_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(cluster_centre_points):\n",
    "    X_train_raw = X_train.todense()\n",
    "    \n",
    "    print(\"Fitting the model............\")\n",
    "    for l in range(1,20):\n",
    "        cluster = [[],[],[],[],[]]\n",
    "        for j in range(0,len(X_train_raw)):\n",
    "            minimum_point = int(1e10)\n",
    "            index = 0\n",
    "            for i in range(0,5):\n",
    "                euclid_dist = np.sum(np.square(cluster_centre_points[i]-X_train_raw[j]))\n",
    "                if(euclid_dist < minimum_point):\n",
    "                    minimum_point = euclid_dist\n",
    "                    index = i\n",
    "            cluster[index].append(j)\n",
    "        cluster_centre_points = calculate_cluster_median(cluster)\n",
    "        print(\"Cluster sizes........\")\n",
    "        for i in range (0,5):\n",
    "            print(len(cluster[i]))\n",
    "\n",
    "    main_cluster = cluster\n",
    "    # print(self.main_cluster)\n",
    "    print(\"Model Fitting Done........\")\n",
    "    return cluster_centre_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_clusters():\n",
    "    cluster_labels = {1:None,2:None,3:None,4:None,5:None}\n",
    "\n",
    "    print(\"Labelling of clusters started.........\")\n",
    "\n",
    "    for i in range(0,len(main_cluster)):\n",
    "        my_dict = {1:0,2:0,3:0,4:0,5:0}\n",
    "        for j in range(len(main_cluster[i])):\n",
    "            my_dict[y_train[main_cluster[i][j]]] = my_dict[y_train[main_cluster[i][j]]]+1 \n",
    "\n",
    "        # print(my_dict)\n",
    "        max_label = -1e10\n",
    "        index = 0\n",
    "        for j in range(1,6):\n",
    "            if(my_dict[j] > max_label):\n",
    "                max_label = my_dict[j]\n",
    "                index = j\n",
    "        cluster_labels[i+1] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(X_test,final_centroid):\n",
    "    print(\"predictions...................\")\n",
    "    predictions = []\n",
    "#     y_true_labels,docs_true = read_files(url)\n",
    "    # print(X_test.shape)\n",
    "\n",
    "    X_test_raw = X_test.todense()\n",
    "\n",
    "    predictions = []\n",
    "    for j in range(0,len(X_test_raw)):\n",
    "        min_dist = 1e10\n",
    "        index = 0\n",
    "        for i in range(0,5):\n",
    "            euclid_dist = np.sum(np.square(final_centroid[i]-X_test_raw[j]))\n",
    "            if(euclid_dist < min_dist):\n",
    "                min_dist = euclid_dist\n",
    "                index = i+1\n",
    "        predictions.append(index)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Reading Done\n",
      "Preprocessing done\n",
      "Fitting the model............\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "223\n",
      "75\n",
      "534\n",
      "280\n",
      "213\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "293\n",
      "86\n",
      "443\n",
      "335\n",
      "168\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "271\n",
      "106\n",
      "429\n",
      "356\n",
      "163\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "271\n",
      "112\n",
      "417\n",
      "360\n",
      "165\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "273\n",
      "94\n",
      "429\n",
      "357\n",
      "172\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "273\n",
      "83\n",
      "451\n",
      "338\n",
      "180\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "274\n",
      "60\n",
      "485\n",
      "321\n",
      "185\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "273\n",
      "49\n",
      "514\n",
      "302\n",
      "187\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "273\n",
      "46\n",
      "526\n",
      "291\n",
      "189\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "273\n",
      "45\n",
      "536\n",
      "281\n",
      "190\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "273\n",
      "45\n",
      "551\n",
      "266\n",
      "190\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "273\n",
      "45\n",
      "569\n",
      "248\n",
      "190\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "273\n",
      "45\n",
      "593\n",
      "223\n",
      "191\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "273\n",
      "45\n",
      "614\n",
      "200\n",
      "193\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "273\n",
      "45\n",
      "623\n",
      "190\n",
      "194\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "273\n",
      "45\n",
      "628\n",
      "184\n",
      "195\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "273\n",
      "45\n",
      "630\n",
      "181\n",
      "196\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "273\n",
      "45\n",
      "630\n",
      "181\n",
      "196\n",
      "calculating cluster centroid........\n",
      "Cluster sizes........\n",
      "273\n",
      "45\n",
      "630\n",
      "181\n",
      "196\n",
      "Model Fitting Done........\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "label,doc = read_files(url)\n",
    "print(\"File Reading Done\")\n",
    "vectorizer_object = vectorizer.fit(doc)\n",
    "X_train = vectorizer_object.transform(doc)\n",
    "print(\"Preprocessing done\")\n",
    "y_train = label\n",
    "cluster_centre_points = initial_cluster_centroid_random()\n",
    "final_centroid = fit(cluster_centre_points)\n",
    "# label_clusters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions...................\n",
      "[3, 3, 3, 3, 1, 1, 1, 3, 3, 1, 3, 1, 1, 1, 1, 3, 3, 1, 3, 1, 1, 3, 3, 1, 3, 3, 4, 3, 1, 4, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 1, 1, 1, 3, 2, 3, 1, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 1, 1, 4, 1, 1, 3, 3, 3, 1, 3, 1, 3, 5, 1, 4, 1, 3, 1, 1, 4, 5, 1, 1, 1, 3, 1, 4, 1, 1, 3, 4, 1, 3, 1, 1, 1, 3, 3, 1, 3, 4, 3, 3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 4, 3, 4, 3, 4, 1, 1, 3, 1, 3, 4, 1, 3, 5, 3, 3, 3, 1, 3, 1, 1, 1, 3, 3, 2, 5, 3, 4, 3, 1, 1, 3, 3, 1, 1, 4, 3, 3, 3, 3, 1, 3, 4, 1, 3, 3, 1, 3, 3, 4, 1, 1, 4, 4, 3, 1, 3, 4, 4, 4, 3, 3, 3, 1, 3, 4, 1, 1, 5, 4, 3, 1, 3, 3, 3, 4, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 5, 2, 1, 3, 3, 1, 1, 4, 3, 3, 1, 3, 5, 3, 3, 3, 3, 3, 1, 5, 3, 3, 4, 1, 3, 3, 3, 1, 1, 3, 3, 1, 3, 1, 3, 3, 3, 4, 5, 1, 4, 3, 3, 1, 3, 3, 1, 3, 3, 5, 5, 3, 3, 3, 1, 3, 3, 1, 3, 1, 3, 3, 1, 5, 3, 1, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 3, 1, 3, 3, 3, 3, 1, 1, 3, 1, 3, 1, 3, 2, 1, 1, 5, 1, 1, 3, 5, 1, 3, 4, 3, 1, 3, 1, 3, 1, 3, 1, 3, 3, 1, 4, 1, 1, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 1, 1, 1, 1, 4, 3, 3, 3, 1, 3, 1, 5, 1, 1, 3, 1, 3, 3, 1, 4, 4, 3, 1, 3, 3, 1, 1, 1, 3, 1, 3, 3, 3, 1, 5, 3, 3, 4, 3, 3, 3, 3, 1, 3, 3, 4, 3, 4, 3, 3, 3, 1, 3, 3, 3, 3, 1, 1, 3, 1, 4, 1, 3, 3, 1]\n",
      "Score == \n",
      "0.786263259447\n"
     ]
    }
   ],
   "source": [
    "X_test = read_test_files(test_url)\n",
    "# X_test = preprocessing_test(X_test)\n",
    "X_test = vectorizer_object.transform(X_test)\n",
    "predictions = cluster(X_test,final_centroid)\n",
    "print(predictions)\n",
    "print(\"Score == \")\n",
    "y_true_labels,doc = read_files(test_url)\n",
    "print(homogeneity_score(predictions,y_true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
